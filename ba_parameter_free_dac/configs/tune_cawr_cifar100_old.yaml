defaults:
  - _self_
  - cluster/local
  - override hydra/sweeper: SMAC

seed: 42
name: tune_cawr_cifar100_${seed}

dacbench_sgd_config:
    seed: ${seed}
    use_validation: false
    use_testing: false
    num_episodes: 1
    epoch_mode: False # in SGDWR paper lr is adapted every mini batch
    cutoff: 18750 # 50000*0.8=40000 -> 40000/64 = 625 -> 30 episodes = 30*625=18750
    dataset_name: CIFAR100
    fraction_of_dataset: 1.0
    # torch_hub_model: ["chenyaofo_pytorch-cifar-models_master", "cifar10_resnet20", False]
    # custom_model: wrn16-8
    custom_model: densenet121
    instance_set_path: ../../instance_sets/sgd/sgd_train_100instances.csv

hydra:
  sweeper:
    smac_class: smac.facade.algorithm_configuration_facade.AlgorithmConfigurationFacade
    scenario:
      seed: ${seed}
      n_trials: 300
      deterministic: true
      n_workers: 64
    smac_kwargs:
      dask_client:
        _target_: dask.distributed.Client
        address: ${create_cluster:${cluster},${hydra.sweeper.scenario.n_workers}}
      logging_level: 20  # 10 DEBUG, 20 INFO
    search_space:  # TODO adjust search space
      hyperparameters:
        T0:
          type: uniform_int
          lower: 0
          upper: 50
          default: 10
          log: false
        eta_min:
          type: uniform_float
          lower: 0
          upper: 0.005
          default: 0
          log: false
        base_lr:
          type: uniform_float
          lower: 0
          upper: 0.005
          default: 0.005
          log: false
        t_mult:
          type: uniform_int
          lower: 1
          upper: 5
          default: 2
          log: false
  run:
    dir: results/tune_cawr_cifar100_${seed}
  sweep:
    dir: results/tune_cawr_cifar100_${seed}
  job:
    chdir: true

T0: 10
eta_min: 0
base_lr: 0.0025
t_mult: 2