{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micha/anaconda3/envs/PFDAC/lib/python3.10/site-packages/dacbench/envs/__init__.py:35: UserWarning: CMA-ES Benchmark not installed. If you want to use this benchmark, please follow the installation guide.\n",
      "  warnings.warn(  # noqa: B028\n",
      "/home/micha/anaconda3/envs/PFDAC/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DACBench Gym registration failed - make sure you have all dependencies installed and their instance sets in the right path!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micha/anaconda3/envs/PFDAC/lib/python3.10/site-packages/dacbench/benchmarks/__init__.py:26: UserWarning: CMA-ES Benchmark not installed. If you want to use this benchmark, please follow the installation guide.\n",
      "  warnings.warn(  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from inspect import signature\n",
    "from matplotlib import pyplot as plt\n",
    "from dacbench.logger import Logger, log2dataframe, load_logs\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from plotting import _plot_performance_over_time, plot_performance_over_time, plot_final_performance_comparison, plot_improvement_probability, plot_configuration_footprint, plot_hp_importance, plot_deepcave\n",
    "#from plotting import plot_performance_over_time, plot_final_performance_comparison, plot_improvement_probability, plot_configuration_footprint, plot_hp_importance, plot_deepcave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDataFrame(glob_path, col_name, col_index, method=None):\n",
    "    d =  [log2dataframe(load_logs(path)) for path in glob.glob(glob_path)]\n",
    "    \n",
    "    df = pd.DataFrame([s[col_name][col_index] for s in d]).transpose()\n",
    "    df['epoch'] = df.index + 1\n",
    "    df = df.melt(id_vars=['epoch'])\n",
    "    df.rename(columns={'variable': 'seed'}, inplace=True)\n",
    "    \n",
    "    if method:\n",
    "        df['method'] = method\n",
    "\n",
    "    return df\n",
    "\n",
    "def toChunkedDataFrame(glob_path, col_name, col_index, chunk_size, method=None):\n",
    "    d = [log2dataframe(load_logs(path)) for path in glob.glob(glob_path)]\n",
    "    \n",
    "    # Create a DataFrame where each column is the series of values from a run.\n",
    "    df = pd.DataFrame([s[col_name][col_index] for s in d]).transpose()\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        arr = df[col].values\n",
    "        n = len(arr)\n",
    "        remainder = n % chunk_size\n",
    "        # If the length isn't a multiple of chunk_size, pad it.\n",
    "        if remainder != 0:\n",
    "            pad_length = chunk_size - remainder\n",
    "            # Use the mean of the last (incomplete) chunk for padding.\n",
    "            pad_value = arr[-remainder:].mean()\n",
    "            padded_arr = np.concatenate([arr, np.full(pad_length, pad_value)])\n",
    "        else:\n",
    "            padded_arr = arr\n",
    "        reshaped_data = padded_arr.reshape(-1, chunk_size)\n",
    "        result_df[col] = reshaped_data.mean(axis=1)\n",
    "    \n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "    result_df['epoch'] = result_df.index + 1\n",
    "    result_df = result_df.melt(id_vars=['epoch'])\n",
    "    result_df.rename(columns={'variable': 'seed'}, inplace=True)\n",
    "    \n",
    "    if method:\n",
    "        result_df['method'] = method\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_final_labels_and_stats(data, epoch_col=\"epoch\", value_col=\"value\", group_col=\"method\",\n",
    "                               label_format=\"{label} ({mean:.3f} SE {sem:.3f})\"):\n",
    "    \"\"\"\n",
    "    Computes final aggregated stats and creates new legend labels, preserving the original order.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        DataFrame containing the data with columns for epoch, value, and group identifier.\n",
    "    epoch_col : str, default \"epoch\"\n",
    "        Column name representing the epoch.\n",
    "    value_col : str, default \"value\"\n",
    "        Column name for the metric values.\n",
    "    group_col : str, default \"method\"\n",
    "        Column name used to group the data.\n",
    "    label_format : str, default \"{label}: {mean:.3f} Â± {sem:.3f}\"\n",
    "        A format string used to create new legend labels, where {label} is the group,\n",
    "        {mean} is the computed mean, and {sem} is the standard error.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    new_labels : list of str\n",
    "        A list of updated legend labels with the final aggregated values in the original order.\n",
    "    final_stats : pd.DataFrame\n",
    "        A DataFrame indexed by the group column containing the mean and standard error,\n",
    "        reindexed to preserve the original order of appearance.\n",
    "    \"\"\"\n",
    "    # Determine the final epoch\n",
    "    final_epoch = data[epoch_col].max()\n",
    "\n",
    "    # Filter data to only include the final epoch\n",
    "    final_data = data[data[epoch_col] == final_epoch]\n",
    "\n",
    "    # Compute aggregated statistics (mean and standard error) grouped by the given group column\n",
    "    final_stats = final_data.groupby(group_col)[value_col].agg([\"mean\", \"sem\"])\n",
    "\n",
    "    # Get the original order of methods as they first appear in the data\n",
    "    original_order = data[group_col].drop_duplicates().tolist()\n",
    "\n",
    "    # Reindex final_stats to follow the original order (dropping any methods that might be missing)\n",
    "    final_stats = final_stats.reindex(original_order)\n",
    "\n",
    "    # Create new labels in the preserved order\n",
    "    new_labels = []\n",
    "    for label in original_order:\n",
    "        if label in final_stats.index and pd.notnull(final_stats.loc[label, \"mean\"]):\n",
    "            new_label = label_format.format(label=label,\n",
    "                                            mean=final_stats.loc[label, \"mean\"],\n",
    "                                            sem=final_stats.loc[label, \"sem\"])\n",
    "        else:\n",
    "            new_label = label\n",
    "        new_labels.append(new_label)\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "sns.set_style('whitegrid')\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "palette = sns.color_palette([palette[0], palette[4], palette[2], palette[5], palette[6]])\n",
    "# sns.set_palette(palette)\n",
    "sns.set_context(\"notebook\", font_scale=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method      value  best_value      diff\n",
      "0          Adam  66.237438   67.038387  0.800949\n",
      "1          CAWR  60.004690   67.038387  7.033698\n",
      "2         COCOB  62.322306   67.038387  4.716081\n",
      "3  D-Adaptation  65.177936   67.038387  1.860451\n",
      "4          DoWG  61.042913   67.038387  5.995474\n",
      "5       Prodigy  67.038387   67.038387  0.000000\n",
      "6    SMAC Fixed  63.447498   67.038387  3.590889\n",
      "7    Tuned CAWR  65.641463   67.038387  1.396924\n"
     ]
    }
   ],
   "source": [
    "smacfixed_va = toDataFrame('../results_cluster/results/tuned_smacfixed_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='SMAC Fixed')\n",
    "cawr_va = toDataFrame('../results_cluster/results/CAWR_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='CAWR')\n",
    "tuned_cawr_va = toDataFrame('../results_cluster/results/tuned_CAWR_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Tuned CAWR')\n",
    "adam_fixed_va = toDataFrame('../results_cluster/results/Adam_fixed_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "prodigy_va = toDataFrame('../results_cluster/results/prodigy_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "cocob_va = toDataFrame('../results_cluster/results/COCOB_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='COCOB')\n",
    "dadaptation_va = toDataFrame('../results_cluster/results/dadaptation_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='D-Adaptation')\n",
    "# prodigy_va = toDataFrame('../results_cluster/results/prodigy_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "# adam_fixed_va = toDataFrame('../results_cluster/results/Adam_fixed_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "dowg_va = toDataFrame('../results_cluster/results/DoWG_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'DoWG')\n",
    "\n",
    "\n",
    "data = pd.concat([adam_fixed_va, smacfixed_va, cawr_va, tuned_cawr_va, prodigy_va, cocob_va, dowg_va, dadaptation_va], ignore_index=True)\n",
    "data['value'] = data.groupby(['method', 'seed'])['value'] \\\n",
    "                        .transform(lambda x: x.ewm(alpha=0.3, adjust=False).mean())\n",
    "                        # .transform(lambda x: savgol_filter(x, window_length=11, polyorder=3))\n",
    "data['value'] = data['value'] * 100 # convert to percent\n",
    "data = data.loc[\n",
    "    data.groupby([\"method\", \"seed\"])[\"epoch\"].idxmax()\n",
    "].reset_index(drop=True)\n",
    "data = data.groupby('method')['value'].mean().reset_index()\n",
    "\n",
    "# 3. For each dataset, compute the best (highest) accuracy,\n",
    "#    then compute the difference for each method as (best_value - current_value).\n",
    "data[\"best_value\"] = data[\"value\"].max()\n",
    "data[\"diff\"] = (data[\"best_value\"] - data[\"value\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method      value  best_value      diff\n",
      "0          Adam  91.143940   91.200396  0.056456\n",
      "1          CAWR  90.487200   91.200396  0.713196\n",
      "2         COCOB  90.008404   91.200396  1.191992\n",
      "3  D-Adaptation  90.789780   91.200396  0.410615\n",
      "4          DoWG  90.661834   91.200396  0.538562\n",
      "5       Prodigy  91.200396   91.200396  0.000000\n",
      "6    SMAC Fixed  90.591467   91.200396  0.608929\n",
      "7    Tuned CAWR  90.957558   91.200396  0.242838\n"
     ]
    }
   ],
   "source": [
    "smacfixed_va = toDataFrame('../results_cluster/results/tuned_smacfixed_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='SMAC Fixed')\n",
    "cawr_va = toDataFrame('../results_cluster/results/CAWR_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='CAWR')\n",
    "tuned_cawr_va = toDataFrame('../results_cluster/results/tuned_CAWR_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Tuned CAWR')\n",
    "adam_fixed_va = toDataFrame('../results_cluster/results/Adam_fixed_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "prodigy_va = toDataFrame('../results_cluster/results/prodigy_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "cocob_va = toDataFrame('../results_cluster/results/COCOB_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='COCOB')\n",
    "dadaptation_va = toDataFrame('../results_cluster/results/dadaptation_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='D-Adaptation')\n",
    "# prodigy_va = toDataFrame('../results_cluster/results/prodigy_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "# adam_fixed_va = toDataFrame('../results_cluster/results/Adam_fixed_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "dowg_va = toDataFrame('../results_cluster/results/DoWG_cifar10/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'DoWG')\n",
    "\n",
    "\n",
    "data = pd.concat([adam_fixed_va, smacfixed_va, cawr_va, tuned_cawr_va, prodigy_va, cocob_va, dowg_va, dadaptation_va], ignore_index=True)\n",
    "data['value'] = data.groupby(['method', 'seed'])['value'] \\\n",
    "                        .transform(lambda x: x.ewm(alpha=0.3, adjust=False).mean())\n",
    "                        # .transform(lambda x: savgol_filter(x, window_length=11, polyorder=3))\n",
    "data['value'] = data['value'] * 100 # convert to percent\n",
    "data = data.loc[\n",
    "    data.groupby([\"method\", \"seed\"])[\"epoch\"].idxmax()\n",
    "].reset_index(drop=True)\n",
    "data = data.groupby('method')['value'].mean().reset_index()\n",
    "\n",
    "# 3. For each dataset, compute the best (highest) accuracy,\n",
    "#    then compute the difference for each method as (best_value - current_value).\n",
    "data[\"best_value\"] = data[\"value\"].max()\n",
    "data[\"diff\"] = (data[\"best_value\"] - data[\"value\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method      value  best_value       diff\n",
      "0          Adam  29.282484   31.565147   2.282663\n",
      "1          CAWR   7.168533   31.565147  24.396613\n",
      "2         COCOB  31.565147   31.565147   0.000000\n",
      "3  D-Adaptation  28.404704   31.565147   3.160442\n",
      "4          DoWG  28.050372   31.565147   3.514774\n",
      "5       Prodigy  31.419111   31.565147   0.146036\n",
      "6    SMAC Fixed  29.888256   31.565147   1.676891\n",
      "7    Tuned CAWR   5.511010   31.565147  26.054136\n"
     ]
    }
   ],
   "source": [
    "smacfixed_va = toDataFrame('../results_cluster/results/dtd2/tuned_smacfixed_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='SMAC Fixed')\n",
    "cawr_va = toDataFrame('../results_cluster/results/dtd2/CAWR_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='CAWR')\n",
    "tuned_cawr_va = toDataFrame('../results_cluster/results/dtd2/tuned_CAWR_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Tuned CAWR')\n",
    "adam_fixed_va = toDataFrame('../results_cluster/results/dtd2/Adam_fixed_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "prodigy_va = toDataFrame('../results_cluster/results/dtd2/prodigy_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "cocob_va = toDataFrame('../results_cluster/results/dtd2/COCOB_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='COCOB')\n",
    "dadaptation_va = toDataFrame('../results_cluster/results/dtd2/dadaptation_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='D-Adaptation')\n",
    "# prodigy_va = toDataFrame('../results_cluster/results/prodigy_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "# adam_fixed_va = toDataFrame('../results_cluster/results/Adam_fixed_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "dowg_va = toDataFrame('../results_cluster/results/dtd2/DoWG_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'DoWG')\n",
    "\n",
    "\n",
    "data = pd.concat([adam_fixed_va, smacfixed_va, cawr_va, tuned_cawr_va, prodigy_va, cocob_va, dowg_va, dadaptation_va], ignore_index=True)\n",
    "data['value'] = data.groupby(['method', 'seed'])['value'] \\\n",
    "                        .transform(lambda x: x.ewm(alpha=0.3, adjust=False).mean())\n",
    "                        # .transform(lambda x: savgol_filter(x, window_length=11, polyorder=3))\n",
    "data['value'] = data['value'] * 100 # convert to percent\n",
    "data = data.loc[\n",
    "    data.groupby([\"method\", \"seed\"])[\"epoch\"].idxmax()\n",
    "].reset_index(drop=True)\n",
    "data = data.groupby('method')['value'].mean().reset_index()\n",
    "\n",
    "# 3. For each dataset, compute the best (highest) accuracy,\n",
    "#    then compute the difference for each method as (best_value - current_value).\n",
    "data[\"best_value\"] = data[\"value\"].max()\n",
    "data[\"diff\"] = (data[\"best_value\"] - data[\"value\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            method  epoch      value   dataset\n",
      "0       Adam_fixed    300  66.237438  cifar100\n",
      "1  tuned_smacfixed    300  63.447498  cifar100\n",
      "2             CAWR    300  60.004690  cifar100\n",
      "3       tuned_CAWR    300  65.641463  cifar100\n",
      "4      dadaptation    300  65.177936  cifar100\n",
      "5          prodigy    300  67.038387  cifar100\n",
      "6            COCOB    300  62.322306  cifar100\n",
      "7             DoWG    300  61.042913  cifar100\n",
      "    dataset           method  baseline_performance  performance_without  \\\n",
      "0  cifar100       Adam_fixed             63.864079            63.525028   \n",
      "1  cifar100  tuned_smacfixed             63.864079            63.923591   \n",
      "2  cifar100             CAWR             63.864079            64.415420   \n",
      "3  cifar100       tuned_CAWR             63.864079            63.610167   \n",
      "4  cifar100      dadaptation             63.864079            63.676385   \n",
      "5  cifar100          prodigy             63.864079            63.410606   \n",
      "6  cifar100            COCOB             63.864079            64.084332   \n",
      "7  cifar100             DoWG             63.864079            64.267103   \n",
      "\n",
      "   marginal_contribution  \n",
      "0               0.339051  \n",
      "1              -0.059512  \n",
      "2              -0.551341  \n",
      "3               0.253912  \n",
      "4               0.187694  \n",
      "5               0.453473  \n",
      "6              -0.220253  \n",
      "7              -0.403024  \n"
     ]
    }
   ],
   "source": [
    "methods = [\"Adam_fixed\", \"tuned_smacfixed\", \"CAWR\", \"tuned_CAWR\", \"dadaptation\", \"prodigy\", \"COCOB\", \"DoWG\"]\n",
    "datasets = [\"cifar100\", ]\n",
    "#\"cifar10\", \"cifar100\"\n",
    "\n",
    "# smacfixed_va = toDataFrame('../results_cluster/results/dtd2/tuned_smacfixed_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='SMAC Fixed')\n",
    "# cawr_va = toDataFrame('../results_cluster/results/dtd2/CAWR_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='CAWR')\n",
    "# tuned_cawr_va = toDataFrame('../results_cluster/results/dtd2/tuned_CAWR_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Tuned CAWR')\n",
    "# adam_fixed_va = toDataFrame('../results_cluster/results/dtd2/Adam_fixed_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "# prodigy_va = toDataFrame('../results_cluster/results/dtd2/prodigy_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "# cocob_va = toDataFrame('../results_cluster/results/dtd2/COCOB_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='COCOB')\n",
    "# dadaptation_va = toDataFrame('../results_cluster/results/dtd2/dadaptation_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='D-Adaptation')\n",
    "# # prodigy_va = toDataFrame('../results_cluster/results/prodigy_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method='Prodigy')\n",
    "# # adam_fixed_va = toDataFrame('../results_cluster/results/Adam_fixed_cifar100/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'Adam')\n",
    "# dowg_va = toDataFrame('../results_cluster/results/dtd2/DoWG_dtd/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, 'DoWG')\n",
    "\n",
    "def transform(df, dataset):\n",
    "    df['value'] = df.groupby(['method', 'seed'])['value'] \\\n",
    "                        .transform(lambda x: x.ewm(alpha=0.3, adjust=False).mean())\n",
    "    df['value'] = df['value'] * 100\n",
    "    df = df.groupby([\"method\", \"epoch\"], as_index=False)[\"value\"].mean()\n",
    "    df = df.loc[\n",
    "        df.groupby([\"method\"])[\"epoch\"].idxmax()\n",
    "    ].reset_index(drop=True)\n",
    "    df['dataset'] = dataset\n",
    "\n",
    "    return df\n",
    "\n",
    "df = pd.concat([transform(toDataFrame('../results_cluster/results/' + method + '_' + dataset + '/*/CustomTrackingWrapper.jsonl', 'validation_accuracies', 6, method=method), dataset) for method in methods for dataset in datasets], ignore_index=True)\n",
    "print(df)\n",
    "# At this point, for each dataset-method we have the final validation accuracy in df[\"value\"].\n",
    "# Higher accuracy is better.\n",
    "\n",
    "# --- Compute Marginal Contribution via Shapley Values ---\n",
    "\n",
    "def calculate_marginal_contributions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate marginal contribution per method for each dataset.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input DataFrame containing columns:\n",
    "                       ['dataset', 'method', 'group', 'epoch', 'value']\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame showing marginal contributions for each method per dataset.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Get unique datasets and methods\n",
    "    datasets = df['dataset'].unique()\n",
    "    methods = df['method'].unique()\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        # Filter the DataFrame for the current dataset\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "\n",
    "        # Calculate the performance with all methods (Baseline)\n",
    "        baseline_performance = df_dataset['value'].mean()\n",
    "\n",
    "        for method in methods:\n",
    "            # Filter out the current method to calculate \"without\" performance\n",
    "            df_without_method = df_dataset[df_dataset['method'] != method]\n",
    "\n",
    "            # Calculate the best performance without the current method\n",
    "            if not df_without_method.empty:\n",
    "                performance_without = df_without_method['value'].mean()\n",
    "            else:\n",
    "                performance_without = 0  # If no methods are left\n",
    "\n",
    "            # Calculate the marginal contribution\n",
    "            marginal_contribution = baseline_performance - performance_without\n",
    "\n",
    "            # Store the results\n",
    "            results.append({\n",
    "                'dataset': dataset,\n",
    "                'method': method,\n",
    "                'baseline_performance': baseline_performance,\n",
    "                'performance_without': performance_without,\n",
    "                'marginal_contribution': marginal_contribution\n",
    "            })\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "mcs = calculate_marginal_contributions(df)\n",
    "# mcs = mcs.groupby([\"method\"], as_index=False)[\"marginal_contribution\"].mean()\n",
    "print(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  epoch     value\n",
      "0  D-Adaptation    300  0.000029\n"
     ]
    }
   ],
   "source": [
    "dadaptation_dlrs = toDataFrame('../results_cluster/results/dadaptation_dtd/*/CustomTrackingWrapper.jsonl',\n",
    "                'dlrs', 10, method='D-Adaptation')\n",
    "\n",
    "dadaptation_dlrs = dadaptation_dlrs.groupby([\"method\", \"epoch\"], as_index=False)[\"value\"].mean()\n",
    "\n",
    "# 2. Select the final epoch for each dataset-method-group combination\n",
    "dadaptation_dlrs = dadaptation_dlrs.loc[\n",
    "    dadaptation_dlrs.groupby([\"method\"])[\"epoch\"].idxmax()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(dadaptation_dlrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  epoch     value\n",
      "0  D-Adaptation    300  0.000407\n"
     ]
    }
   ],
   "source": [
    "dadaptation_dlrs = toDataFrame('../results_cluster/results/prodigy_dtd/*/CustomTrackingWrapper.jsonl',\n",
    "                'dlrs', 10, method='D-Adaptation')\n",
    "\n",
    "dadaptation_dlrs = dadaptation_dlrs.groupby([\"method\", \"epoch\"], as_index=False)[\"value\"].mean()\n",
    "\n",
    "# 2. Select the final epoch for each dataset-method-group combination\n",
    "dadaptation_dlrs = dadaptation_dlrs.loc[\n",
    "    dadaptation_dlrs.groupby([\"method\"])[\"epoch\"].idxmax()\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(dadaptation_dlrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFDAC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
